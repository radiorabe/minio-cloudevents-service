{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MinIO CloudEvents Service","text":"<p>MinIO CloudEvents Service consumes MinIO events from a Kafka topic in S3/MinIO format, converts them to CloudEvents, and sends them to a <code>cloudevents</code> topic in Kafka.</p> <p>In essence it's a workaround for the fact that MinIO does not allow natively sending CloudEvents since it implements the same API surface that AWS S3 also provides.</p> <p>To acheive this, we implement a simple S3 CloudEvents Adapter.</p>"},{"location":"#usage","title":"Usage","text":"<p>Run the container to see all available configuration options and their corresponding environment variables.</p> <pre><code>podman run --rm ghcr.io/radiorabe/minioevents:latest --help\n</code></pre>"},{"location":"#development","title":"Development","text":"<pre><code>python -mvenv venv\n. venv/bin/activate\n\npip install poetry\npoetry install\n\npytest\n</code></pre>"},{"location":"#release-management","title":"Release Management","text":"<p>The CI/CD setup uses semantic commit messages following the conventional commits standard. The workflow is based on the RaBe shared actions and uses go-semantic-commit to create new releases.</p> <p>The commit message should be structured as follows:</p> <pre><code>&lt;type&gt;[optional scope]: &lt;description&gt;\n\n[optional body]\n\n[optional footer(s)]\n</code></pre> <p>The commit contains the following structural elements, to communicate intent to the consumers of your library:</p> <ol> <li>fix: a commit of the type <code>fix</code> patches gets released with a PATCH version bump</li> <li>feat: a commit of the type <code>feat</code> gets released as a MINOR version bump</li> <li>BREAKING CHANGE: a commit that has a footer <code>BREAKING CHANGE:</code> gets released as a MAJOR version bump</li> <li>types other than <code>fix:</code> and <code>feat:</code> are allowed and don't trigger a release</li> </ol> <p>If a commit does not contain a conventional commit style message you can fix it during the squash and merge operation on the PR.</p>"},{"location":"#build-process","title":"Build Process","text":"<p>The CI/CD setup uses Docker build-push Action  to publish container images. The workflow is based on the RaBe shared actions.</p>"},{"location":"#license","title":"License","text":"<p>This application is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, version 3 of the License.</p>"},{"location":"#copyright","title":"Copyright","text":"<p>Copyright (c) 2023 Radio Bern RaBe</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>minioevents</li> </ul>"},{"location":"reference/minioevents/","title":"minioevents","text":""},{"location":"reference/minioevents/#minioevents.app","title":"<code>app(bootstrap_servers, security_protocol, tls_cafile, tls_certfile, tls_keyfile, consumer_topic, consumer_group, consumer_auto_offset_reset, producer_topic)</code>","text":"<p>Set up kafka consumer and producer, block until a SIGINT while processing messages.</p> Source code in <code>minioevents.py</code> <pre><code>def app(\nbootstrap_servers: list[str],\nsecurity_protocol: str,\ntls_cafile: str,\ntls_certfile: str,\ntls_keyfile: str,\nconsumer_topic: str,\nconsumer_group: str,\nconsumer_auto_offset_reset: str,\nproducer_topic: str,\n):\n\"\"\"\n    Set up kafka consumer and producer, block until a SIGINT while processing messages.\n    \"\"\"\nconsumer = KafkaConsumer(\nconsumer_topic,\nbootstrap_servers=bootstrap_servers,\nsecurity_protocol=security_protocol,\ngroup_id=consumer_group,\nauto_offset_reset=consumer_auto_offset_reset,\nssl_cafile=tls_cafile,\nssl_certfile=tls_certfile,\nssl_keyfile=tls_keyfile,\n)\nproducer = KafkaProducer(\nbootstrap_servers=bootstrap_servers,\nsecurity_protocol=security_protocol,\nretries=5,\nmax_in_flight_requests_per_connection=1,\nkey_serializer=lambda k: bytes(k, \"utf-8\"),\nssl_cafile=tls_cafile,\nssl_certfile=tls_certfile,\nssl_keyfile=tls_keyfile,\n)\ndef on_sigint(*_):  # pragma: no cover\nconsumer.close()\nproducer.flush()\nproducer.close()\nsys.exit(0)\nsignal.signal(signal.SIGINT, on_sigint)\ndef on_send_error(ex):  # pragma: no cover\nlogger.error(\"Failed to send CloudEvent\", exc_info=ex)\nfor msg in consumer:\nfor ce in from_consumer_record(msg):\nkm = to_structured(\nce,\nkey_mapper=lambda event: \".\".join(\n[\nce.get(\"type\"),\nce.get(\"source\"),\nce.get(\"subject\"),\n]\n),\n)\nproducer.send(\nproducer_topic,\nkey=km.key,\nvalue=km.value,\nheaders=km.headers if km.headers else None,\n).add_errback(on_send_error)\nproducer.flush()\n</code></pre>"},{"location":"reference/minioevents/#minioevents.from_consumer_record","title":"<code>from_consumer_record(msg)</code>","text":"<p>Convert msg to an array of CloudEvents using a naive implementation of https://github.com/cloudevents/spec/blob/main/cloudevents/adapters/aws-s3.md.</p> Source code in <code>minioevents.py</code> <pre><code>def from_consumer_record(msg: ConsumerRecord) -&gt; [CloudEvent]:\n\"\"\"\n    Convert msg to an array of CloudEvents using a naive implementation of https://github.com/cloudevents/spec/blob/main/cloudevents/adapters/aws-s3.md.\n    \"\"\"\nfor rec in json.loads(msg.value).get(\"Records\", []):\nyield CloudEvent(\n{\n\"id\": \".\".join(\n[\nrec.get(\"responseElements\", {}).get(\"x-amz-request-id\"),\nrec.get(\"responseElements\", {}).get(\"x-amz-id-2\"),\n]\n),\n\"source\": \".\".join(\n[\nrec.get(\"eventSource\"),\nrec.get(\"awsRegion\"),\nrec.get(\"s3\", {}).get(\"bucket\", {}).get(\"name\"),\n]\n),\n\"specversion\": \"1.0\",\n\"type\": \".\".join(\n[\n\"com.amazonaws.s3\",\nrec.get(\"eventName\"),\n]\n),\n\"datacontenttype\": \"application/json\",\n\"subject\": rec.get(\"s3\", {}).get(\"object\", {}).get(\"key\"),\n\"time\": rec.get(\"eventTime\"),\n},\nrec,\n)\n</code></pre>"},{"location":"reference/minioevents/#minioevents.main","title":"<code>main()</code>","text":"<p>CLI entrypoint parses args, sets up logging, and calls <code>app()</code>.</p> Source code in <code>minioevents.py</code> <pre><code>def main():  # pragma: no cover\n\"\"\"\n    CLI entrypoint parses args, sets up logging, and calls `app()`.\n    \"\"\"\nparser = ArgumentParser(__name__)\nparser.add(\n\"--kafka-bootstrap-servers\",\nrequired=True,\nenv_var=\"KAFKA_BOOTSTRAP_SERVERS\",\n)\nparser.add(\n\"--kafka-security-protocol\",\ndefault=\"PLAINTEXT\",\nenv_var=\"KAFKA_SECURITY_PROTOCOL\",\n)\nparser.add(\n\"--kafka-tls-cafile\",\ndefault=None,\nenv_var=\"KAFKA_TLS_CAFILE\",\n)\nparser.add(\n\"--kafka-tls-certfile\",\ndefault=None,\nenv_var=\"KAFKA_TLS_CERTFILE\",\n)\nparser.add(\n\"--kafka-tls-keyfile\",\ndefault=None,\nenv_var=\"KAFKA_TLS_KEYFILE\",\n)\nparser.add(\n\"--kafka-consumer-topic\",\ndefault=\"minioevents\",\nenv_var=\"KAFKA_CONSUMER_TOPIC\",\n)\nparser.add(\n\"--kafka-consumer-group\",\ndefault=__name__,\nenv_var=\"KAFKA_CONSUMER_GROUP\",\n)\nparser.add(\n\"--kafka-consumer-auto-offset-reset\",\ndefault=\"latest\",\nenv_var=\"KAFKA_CONSUMER_AUTO_OFFSET_RESET\",\n)\nparser.add(\n\"--kafka-producer-topic\",\ndefault=\"cloudevents\",\nenv_var=\"KAFKA_PRODUCER_TOPIC\",\n)\nparser.add(\n\"--quiet\",\n\"-q\",\ndefault=False,\naction=\"store_true\",\nenv_var=\"MINIOEVENTS_QUIET\",\n)\noptions = parser.parse_args()\nif not options.quiet:\nlogging.basicConfig(level=logging.INFO)\nlogger.info(f\"Starting {__name__}...\")\napp(\nbootstrap_servers=options.kafka_bootstrap_servers,\nsecurity_protocol=options.kafka_security_protocol,\ntls_cafile=options.kafka_tls_cafile,\ntls_certfile=options.kafka_tls_certfile,\ntls_keyfile=options.kafka_tls_keyfile,\nconsumer_topic=options.kafka_consumer_topic,\nconsumer_group=options.kafka_consumer_group,\nconsumer_auto_offset_reset=options.kafka_consumer_auto_offset_reset,\nproducer_topic=options.kafka_producer_topic,\n)\n</code></pre>"}]}