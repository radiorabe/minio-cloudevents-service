{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MinIO CloudEvents Service","text":"<p>MinIO CloudEvents Service consumes MinIO events from a Kafka topic in S3/MinIO format, converts them to CloudEvents, and sends them to a <code>cloudevents</code> topic in Kafka.</p> <p>In essence it's a workaround for the fact that MinIO does not allow natively sending CloudEvents since it implements the same API surface that AWS S3 also provides.</p> <p>To acheive this, we implement a simple S3 CloudEvents Adapter.</p>"},{"location":"#usage","title":"Usage","text":"<p>Run the container to see all available configuration options and their corresponding environment variables.</p> <pre><code>podman run --rm ghcr.io/radiorabe/minioevents:latest --help\n</code></pre>"},{"location":"#development","title":"Development","text":"<pre><code>python -mvenv venv\n. venv/bin/activate\n\npip install poetry\npoetry install\n\npytest\n</code></pre>"},{"location":"#release-management","title":"Release Management","text":"<p>The CI/CD setup uses semantic commit messages following the conventional commits standard. The workflow is based on the RaBe shared actions and uses go-semantic-commit to create new releases.</p> <p>The commit message should be structured as follows:</p> <pre><code>&lt;type&gt;[optional scope]: &lt;description&gt;\n\n[optional body]\n\n[optional footer(s)]\n</code></pre> <p>The commit contains the following structural elements, to communicate intent to the consumers of your library:</p> <ol> <li>fix: a commit of the type <code>fix</code> patches gets released with a PATCH version bump</li> <li>feat: a commit of the type <code>feat</code> gets released as a MINOR version bump</li> <li>BREAKING CHANGE: a commit that has a footer <code>BREAKING CHANGE:</code> gets released as a MAJOR version bump</li> <li>types other than <code>fix:</code> and <code>feat:</code> are allowed and don't trigger a release</li> </ol> <p>If a commit does not contain a conventional commit style message you can fix it during the squash and merge operation on the PR.</p>"},{"location":"#build-process","title":"Build Process","text":"<p>The CI/CD setup uses Docker build-push Action  to publish container images. The workflow is based on the RaBe shared actions.</p>"},{"location":"#license","title":"License","text":"<p>This application is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, version 3 of the License.</p>"},{"location":"#copyright","title":"Copyright","text":"<p>Copyright (c) 2023 Radio Bern RaBe</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>minioevents</li> </ul>"},{"location":"reference/minioevents/","title":"minioevents","text":""},{"location":"reference/minioevents/#minioevents.app","title":"<code>app(bootstrap_servers, security_protocol, tls_cafile, tls_certfile, tls_keyfile, consumer_topic, consumer_group, consumer_auto_offset_reset, producer_topic)</code>","text":"<p>Set up kafka consumer and producer, block until a SIGINT while processing messages.</p> Source code in <code>minioevents.py</code> <pre><code>def app(\n    bootstrap_servers: list[str],\n    security_protocol: str,\n    tls_cafile: str,\n    tls_certfile: str,\n    tls_keyfile: str,\n    consumer_topic: str,\n    consumer_group: str,\n    consumer_auto_offset_reset: str,\n    producer_topic: str,\n):\n    \"\"\"\n    Set up kafka consumer and producer, block until a SIGINT while processing messages.\n    \"\"\"\n    consumer = KafkaConsumer(\n        consumer_topic,\n        bootstrap_servers=bootstrap_servers,\n        security_protocol=security_protocol,\n        group_id=consumer_group,\n        auto_offset_reset=consumer_auto_offset_reset,\n        ssl_cafile=tls_cafile,\n        ssl_certfile=tls_certfile,\n        ssl_keyfile=tls_keyfile,\n    )\n    producer = KafkaProducer(\n        bootstrap_servers=bootstrap_servers,\n        security_protocol=security_protocol,\n        retries=5,\n        max_in_flight_requests_per_connection=1,\n        key_serializer=lambda k: bytes(k, \"utf-8\"),\n        ssl_cafile=tls_cafile,\n        ssl_certfile=tls_certfile,\n        ssl_keyfile=tls_keyfile,\n    )\n\n    def on_sigint(*_):  # pragma: no cover\n        consumer.close()\n        producer.flush()\n        producer.close()\n        sys.exit(0)\n\n    signal.signal(signal.SIGINT, on_sigint)\n\n    def on_send_error(ex):  # pragma: no cover\n        logger.error(\"Failed to send CloudEvent\", exc_info=ex)\n\n    for msg in consumer:\n        for ce in from_consumer_record(msg):\n            km = to_structured(\n                ce,\n                key_mapper=lambda event: \".\".join(\n                    [\n                        ce.get(\"type\"),\n                        ce.get(\"source\"),\n                        ce.get(\"subject\"),\n                    ]\n                ),\n            )\n            producer.send(\n                producer_topic,\n                key=km.key,\n                value=km.value,\n                headers=km.headers if km.headers else None,\n            ).add_errback(on_send_error)\n        producer.flush()\n</code></pre>"},{"location":"reference/minioevents/#minioevents.from_consumer_record","title":"<code>from_consumer_record(msg)</code>","text":"<p>Convert msg to an array of CloudEvents using a naive implementation of https://github.com/cloudevents/spec/blob/main/cloudevents/adapters/aws-s3.md.</p> Source code in <code>minioevents.py</code> <pre><code>def from_consumer_record(msg: ConsumerRecord) -&gt; [CloudEvent]:\n    \"\"\"\n    Convert msg to an array of CloudEvents using a naive implementation of https://github.com/cloudevents/spec/blob/main/cloudevents/adapters/aws-s3.md.\n    \"\"\"\n    for rec in json.loads(msg.value).get(\"Records\", []):\n        yield CloudEvent(\n            {\n                \"id\": \".\".join(\n                    [\n                        rec.get(\"responseElements\", {}).get(\"x-amz-request-id\"),\n                        rec.get(\"responseElements\", {}).get(\"x-amz-id-2\"),\n                    ]\n                ),\n                \"source\": \".\".join(\n                    [\n                        rec.get(\"eventSource\"),\n                        rec.get(\"awsRegion\"),\n                        rec.get(\"s3\", {}).get(\"bucket\", {}).get(\"name\"),\n                    ]\n                ),\n                \"specversion\": \"1.0\",\n                \"type\": \".\".join(\n                    [\n                        \"com.amazonaws.s3\",\n                        rec.get(\"eventName\"),\n                    ]\n                ),\n                \"datacontenttype\": \"application/json\",\n                \"subject\": rec.get(\"s3\", {}).get(\"object\", {}).get(\"key\"),\n                \"time\": rec.get(\"eventTime\"),\n            },\n            rec,\n        )\n</code></pre>"},{"location":"reference/minioevents/#minioevents.main","title":"<code>main()</code>","text":"<p>CLI entrypoint parses args, sets up logging, and calls <code>app()</code>.</p> Source code in <code>minioevents.py</code> <pre><code>def main():  # pragma: no cover\n    \"\"\"\n    CLI entrypoint parses args, sets up logging, and calls `app()`.\n    \"\"\"\n    parser = ArgumentParser(__name__)\n    parser.add(\n        \"--kafka-bootstrap-servers\",\n        required=True,\n        env_var=\"KAFKA_BOOTSTRAP_SERVERS\",\n    )\n    parser.add(\n        \"--kafka-security-protocol\",\n        default=\"PLAINTEXT\",\n        env_var=\"KAFKA_SECURITY_PROTOCOL\",\n    )\n    parser.add(\n        \"--kafka-tls-cafile\",\n        default=None,\n        env_var=\"KAFKA_TLS_CAFILE\",\n    )\n    parser.add(\n        \"--kafka-tls-certfile\",\n        default=None,\n        env_var=\"KAFKA_TLS_CERTFILE\",\n    )\n    parser.add(\n        \"--kafka-tls-keyfile\",\n        default=None,\n        env_var=\"KAFKA_TLS_KEYFILE\",\n    )\n    parser.add(\n        \"--kafka-consumer-topic\",\n        default=\"minioevents\",\n        env_var=\"KAFKA_CONSUMER_TOPIC\",\n    )\n    parser.add(\n        \"--kafka-consumer-group\",\n        default=__name__,\n        env_var=\"KAFKA_CONSUMER_GROUP\",\n    )\n    parser.add(\n        \"--kafka-consumer-auto-offset-reset\",\n        default=\"latest\",\n        env_var=\"KAFKA_CONSUMER_AUTO_OFFSET_RESET\",\n    )\n    parser.add(\n        \"--kafka-producer-topic\",\n        default=\"cloudevents\",\n        env_var=\"KAFKA_PRODUCER_TOPIC\",\n    )\n    parser.add(\n        \"--quiet\",\n        \"-q\",\n        default=False,\n        action=\"store_true\",\n        env_var=\"MINIOEVENTS_QUIET\",\n    )\n\n    options = parser.parse_args()\n\n    if not options.quiet:\n        logging.basicConfig(level=logging.INFO)\n    logger.info(f\"Starting {__name__}...\")\n\n    app(\n        bootstrap_servers=options.kafka_bootstrap_servers,\n        security_protocol=options.kafka_security_protocol,\n        tls_cafile=options.kafka_tls_cafile,\n        tls_certfile=options.kafka_tls_certfile,\n        tls_keyfile=options.kafka_tls_keyfile,\n        consumer_topic=options.kafka_consumer_topic,\n        consumer_group=options.kafka_consumer_group,\n        consumer_auto_offset_reset=options.kafka_consumer_auto_offset_reset,\n        producer_topic=options.kafka_producer_topic,\n    )\n</code></pre>"}]}